{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb8c32f7-716b-42be-a43e-7e2ac625dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model from: /opt/ml/outputs/models/no1_xlm_roberta\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n",
      "/opt/ml/outputs/models/no1_xlm_roberta/checkpoint-882\n",
      "data is from /opt/ml/input/data/test_dataset\n",
      "model is from /opt/ml/outputs/models/no1_xlm_roberta/checkpoint-882\n",
      "404 Client Error: Not Found for url: https://huggingface.co/tokenizer_name/resolve/main/config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py\", line 465, in get_config_dict\n",
      "    user_agent=user_agent,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py\", line 1173, in cached_path\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py\", line 1336, in get_from_cache\n",
      "    r.raise_for_status()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/tokenizer_name/resolve/main/config.json\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"inference.py\", line 117, in <module>\n",
      "    main()\n",
      "  File \"inference.py\", line 51, in main\n",
      "    config, tokenizer, model = get_CTM(model_args)\n",
      "  File \"/opt/ml/code/utils.py\", line 43, in get_CTM\n",
      "    use_fast=True\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\", line 390, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/models/auto/configuration_auto.py\", line 398, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py\", line 477, in get_config_dict\n",
      "    raise EnvironmentError(msg)\n",
      "OSError: Can't load config for 'tokenizer_name'. Make sure that:\n",
      "\n",
      "- 'tokenizer_name' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'tokenizer_name' is the correct path to a directory containing a config.json file\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-7817ad1bb06c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprev_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "수동으로 training과 동기화해야 합니다.\n",
    "- training 시작 후, model 저장 폴더(model_output_path)가 생성되면 코드 실행.\n",
    "\"\"\"\n",
    "from IPython.display import display\n",
    "from utils import increment_path\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def display_dots(num_dots):\n",
    "    str_dots = \".\" * num_dots\n",
    "    dot_display.update(str_dots)\n",
    "num_dots = 0\n",
    "dot_display = display(\"\", display_id=True)\n",
    "\n",
    "# ********** CHANGE HERE (1/1): Path of pretrained reader model **********\n",
    "model_output_path = \"/opt/ml/outputs/models/no1_xlm_roberta\"\n",
    "# ************************************************************************\n",
    "print(\"model from:\", model_output_path)\n",
    "eof_dir_path = model_output_path + \"/zeof\"\n",
    "\n",
    "dataset_path = \"/opt/ml/input/data/test_dataset\"\n",
    "prev_model_path = None\n",
    "while True:\n",
    "    num_dots += 1\n",
    "    num_dots %= 10\n",
    "    display_dots(num_dots)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    model_paths = sorted(glob(model_output_path + \"/*\"))\n",
    "    if not model_paths:\n",
    "        continue\n",
    "    model_path = model_paths[-1]\n",
    "    if model_path == eof_dir_path:\n",
    "        print(\"Found EOF\")\n",
    "        break\n",
    "    if model_path == prev_model_path:\n",
    "        continue\n",
    "    \n",
    "    num_dots = 0\n",
    "    display_dots(num_dots)\n",
    "    print(model_path)\n",
    "    prev_model_path = model_path\n",
    "    \n",
    "    result_name = \"_\".join(model_path.split(\"/\")[-2:])\n",
    "    test_output_dir = increment_path(\"/opt/ml/outputs/preds\", \"/no\", result_name)\n",
    "    !python inference.py\\\n",
    "        --output_dir $test_output_dir\\\n",
    "        --dataset_path $dataset_path\\\n",
    "        --model_path $model_path\\\n",
    "        --topk 20\n",
    "    \n",
    "print(\"Inference End.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f66f377-ecfe-4801-8651-6e48c76757f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/all_results.json',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/checkpoint-536',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/checkpoint-603',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/checkpoint-670',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/config.json',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/pytorch_model.bin',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/special_tokens_map.json',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/tokenizer_config.json',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/train_results.json',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/train_results.txt',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/trainer_state.json',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/training_args.bin',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/vocab.txt',\n",
       " '/opt/ml/outputs/models/no1_koelectra-base-v3-finetuned-korquad_from_git/zeof']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(glob(model_output_path + \"/*\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
