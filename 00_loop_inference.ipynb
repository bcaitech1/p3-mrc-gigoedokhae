{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb8c32f7-716b-42be-a43e-7e2ac625dae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "...."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model from: /opt/ml/outputs/models/no8_xlm_roberta_fine_tuning_continue\n",
      "/opt/ml/outputs/models/no8_xlm_roberta_fine_tuning_continue/checkpoint-588\n",
      "data is from /opt/ml/input/data/test_dataset\n",
      "model is from /opt/ml/outputs/models/no8_xlm_roberta_fine_tuning_continue/checkpoint-588\n",
      "model uses device: cuda:0\n",
      "Dataset({\n",
      "    features: ['id', 'question'],\n",
      "    num_rows: 600\n",
      "})\n",
      "Lengths of unique contexts : 56607\n",
      "Passage Embedding Loaded.\n",
      "[Relevant documents exhaustive search.] done in 7.328 s\n",
      "Making DataFrame dataset:: 600it [00:00, 14978.14it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:57<00:00,  4.81s/ba]\n",
      "Init Trainer...\n",
      "100%|███████████████████████████████████████| 2426/2426 [25:41<00:00,  1.51it/s]INFO:utils_qa:Post-processing 12000 example predictions split into 19404 features.\n",
      "100%|████████████████████████████████████| 12000/12000 [01:10<00:00, 170.31it/s]\n",
      "INFO:utils_qa:Saving predictions to /opt/ml/outputs/preds/no17_no8_xlm_roberta_fine_tuning_continue_checkpoint-588/predictions.json.\n",
      "INFO:utils_qa:Saving nbest_preds to /opt/ml/outputs/preds/no17_no8_xlm_roberta_fine_tuning_continue_checkpoint-588/nbest_predictions.json.\n",
      "Job done! (No metric can be presented because there is no correct answer given.)\n",
      "100%|███████████████████████████████████████| 2426/2426 [27:15<00:00,  1.48it/s]\n",
      "/opt/ml/outputs/models/no8_xlm_roberta_fine_tuning_continue/checkpoint-1176\n",
      "data is from /opt/ml/input/data/test_dataset\n",
      "model is from /opt/ml/outputs/models/no8_xlm_roberta_fine_tuning_continue/checkpoint-1176\n",
      "model uses device: cuda:0\n",
      "Dataset({\n",
      "    features: ['id', 'question'],\n",
      "    num_rows: 600\n",
      "})\n",
      "Lengths of unique contexts : 56607\n",
      "Passage Embedding Loaded.\n",
      "[Relevant documents exhaustive search.] done in 5.534 s\n",
      "Making DataFrame dataset:: 600it [00:00, 16358.97it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:55<00:00,  4.62s/ba]\n",
      "Init Trainer...\n",
      "100%|███████████████████████████████████████| 2426/2426 [25:45<00:00,  1.56it/s]INFO:utils_qa:Post-processing 12000 example predictions split into 19404 features.\n",
      "100%|████████████████████████████████████| 12000/12000 [01:10<00:00, 169.12it/s]\n",
      "INFO:utils_qa:Saving predictions to /opt/ml/outputs/preds/no18_no8_xlm_roberta_fine_tuning_continue_checkpoint-1176/predictions.json.\n",
      "INFO:utils_qa:Saving nbest_preds to /opt/ml/outputs/preds/no18_no8_xlm_roberta_fine_tuning_continue_checkpoint-1176/nbest_predictions.json.\n",
      "Job done! (No metric can be presented because there is no correct answer given.)\n",
      "100%|███████████████████████████████████████| 2426/2426 [27:21<00:00,  1.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-f5e841fa5799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnum_dots\u001b[0m \u001b[0;34m%=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdisplay_dots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_dots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmodel_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "수동으로 training과 동기화해야 합니다.\n",
    "- training 시작 후, model 저장 폴더(model_output_path)가 생성되면 입력 후 코드 실행.\n",
    "\"\"\"\n",
    "from IPython.display import display\n",
    "from utils import increment_path\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def str_formatter(string, pp, cycle):\n",
    "    pp.text(string)\n",
    "plain = get_ipython().display_formatter.formatters['text/plain']\n",
    "plain.for_type(str, str_formatter)\n",
    "\n",
    "def display_dots(num_dots):\n",
    "    str_dots = \".\" * num_dots\n",
    "    dot_display.update(str_dots)\n",
    "num_dots = 0\n",
    "dot_display = display(\"\", display_id=True)\n",
    "\n",
    "\n",
    "# ********** CHANGE HERE (1/1): Path of pretrained reader model **********\n",
    "model_output_path = \"/opt/ml/outputs/models/no8_xlm_roberta_fine_tuning_continue\"\n",
    "# ************************************************************************\n",
    "tokenizer_name = \"xlm-roberta-large\" # use huggingface modal name, if \"Can't load tokenizer\"\n",
    "print(\"model from:\", model_output_path)\n",
    "eof_dir_path = model_output_path + \"/zeof\"\n",
    "\n",
    "dataset_path = \"/opt/ml/input/data/test_dataset\"\n",
    "prev_model_path = None\n",
    "while True:\n",
    "    num_dots += 1\n",
    "    num_dots %= 10\n",
    "    display_dots(num_dots)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    model_paths = sorted(glob(model_output_path + \"/*\"))\n",
    "    if not model_paths:\n",
    "        continue\n",
    "    model_path = model_paths[-1]\n",
    "    if model_path == eof_dir_path:\n",
    "        print(\"Found EOF\")\n",
    "        break\n",
    "    if model_path == prev_model_path:\n",
    "        continue\n",
    "    \n",
    "    num_dots = 0\n",
    "    display_dots(num_dots)\n",
    "    print(model_path)\n",
    "    prev_model_path = model_path\n",
    "    \n",
    "    result_name = \"_\".join(model_path.split(\"/\")[-2:])\n",
    "    test_output_dir = increment_path(\"/opt/ml/outputs/preds\", \"/no\", result_name)\n",
    "    !python inference.py\\\n",
    "        --tokenizer_name $tokenizer_name\\\n",
    "        --output_dir $test_output_dir\\\n",
    "        --dataset_path $dataset_path\\\n",
    "        --model_path $model_path\\\n",
    "        --topk 20\n",
    "    \n",
    "print(\"Inference End.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66f377-ecfe-4801-8651-6e48c76757f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
