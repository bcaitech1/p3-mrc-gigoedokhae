{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import increment_path\n",
    "from transformers import logging\n",
    "#logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model is from bert-base-multilingual-cased\n",
      "data is from ../input/data/train_dataset\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['__index_level_0__', 'answers', 'context', 'document_id', 'id', 'question', 'title'],\n",
      "        num_rows: 3952\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['__index_level_0__', 'answers', 'context', 'document_id', 'id', 'question', 'title'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "})\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/28/2021 16:24:57 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../input/data/train_dataset/train/cache-06034a54125b76c1.arrow\n",
      "{'loss': 1.8063, 'learning_rate': 5e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0231, 'learning_rate': 5e-05, 'epoch': 1.68}\n",
      "{'loss': 0.6903, 'learning_rate': 5e-05, 'epoch': 2.52}\n",
      "{'loss': 0.4855, 'learning_rate': 5e-05, 'epoch': 3.36}\n",
      "{'loss': 0.3579, 'learning_rate': 5e-05, 'epoch': 4.2}\n",
      "{'loss': 0.3017, 'learning_rate': 5e-05, 'epoch': 5.04}\n",
      "{'loss': 0.2424, 'learning_rate': 5e-05, 'epoch': 5.88}\n",
      "{'loss': 0.2267, 'learning_rate': 5e-05, 'epoch': 6.72}\n",
      "{'loss': 0.2, 'learning_rate': 5e-05, 'epoch': 7.56}\n",
      "{'loss': 0.1886, 'learning_rate': 5e-05, 'epoch': 8.4}\n",
      "{'loss': 0.1809, 'learning_rate': 5e-05, 'epoch': 9.24}\n",
      "{'train_runtime': 4472.5974, 'train_samples_per_second': 1.33, 'epoch': 10.0}\n",
      "100%|█████████████████████████████████████| 5950/5950 [1:14:32<00:00,  1.33it/s]\n",
      "end training.\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"test_10epochs\"\n",
    "model_output_dir = increment_path(\"./models/train_dataset\", \"/exp\", exp_name)\n",
    "!python train.py\\\n",
    "    --do_train\\\n",
    "    --run_name None\\\n",
    "    --output_dir $model_output_dir\\\n",
    "    --overwrite_output_dir False\\\n",
    "    --seed 42\\\n",
    "    --fp16 True --fp16_opt_level \"O1\"\\\n",
    "    --dataloader_num_workers 4\n",
    "    --dataloader_pin_memory True\\\n",
    "    --dataloader_drop_last False\\\n",
    "    \\\n",
    "    --num_train_epochs 10.0\\\n",
    "    --per_device_train_batch_size 16 --gradient_accumulation_steps 1\\\n",
    "    --per_device_eval_batch_size 16\\\n",
    "    --save_strategy \"epoch\"\\\n",
    "    --save_steps 500 --save_total_limit 3\\\n",
    "    \\\n",
    "    --learning_rate 5e-5 --lr_scheduler_type \"constant_with_warmup\"\\\n",
    "    --warmup_ratio 0.0 --warmup_steps 0\\\n",
    "    --weight_decay 0 --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-8\\\n",
    "    --adafactor False\\\n",
    "    --max_grad_norm 1.0\\\n",
    "    \\\n",
    "    --group_by_length False\\\n",
    "    --label_smoothing_factor 0.0\\\n",
    "    \n",
    "    \n",
    "print(\"end training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model is from /opt/ml/code/models/train_dataset/exp1_test_10epochs\n",
      "data is from ../input/data/train_dataset\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['__index_level_0__', 'answers', 'context', 'document_id', 'id', 'question', 'title'],\n",
      "        num_rows: 3952\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['__index_level_0__', 'answers', 'context', 'document_id', 'id', 'question', 'title'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "})\n",
      "04/28/2021 18:03:18 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at ../input/data/train_dataset/validation/cache-7b9317d119a8210f.arrow\n",
      "100%|███████████████████████████████████████████| 73/73 [00:07<00:00,  9.28it/s]04/28/2021 18:03:32 - INFO - utils_qa -   Post-processing 240 example predictions split into 577 features.\n",
      "\n",
      "  0%|                                                   | 0/240 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▌                                      | 15/240 [00:00<00:01, 142.36it/s]\u001b[A\n",
      " 12%|█████▏                                   | 30/240 [00:00<00:01, 144.43it/s]\u001b[A\n",
      " 19%|███████▊                                 | 46/240 [00:00<00:01, 147.02it/s]\u001b[A\n",
      " 26%|██████████▊                              | 63/240 [00:00<00:01, 151.08it/s]\u001b[A\n",
      " 33%|█████████████▋                           | 80/240 [00:00<00:01, 156.11it/s]\u001b[A\n",
      " 40%|████████████████▏                        | 95/240 [00:00<00:00, 152.57it/s]\u001b[A\n",
      " 46%|██████████████████▌                     | 111/240 [00:00<00:00, 154.25it/s]\u001b[A\n",
      " 53%|█████████████████████▎                  | 128/240 [00:00<00:00, 157.80it/s]\u001b[A\n",
      " 60%|████████████████████████                | 144/240 [00:00<00:00, 146.16it/s]\u001b[A\n",
      " 66%|██████████████████████████▌             | 159/240 [00:01<00:00, 143.64it/s]\u001b[A\n",
      " 72%|█████████████████████████████           | 174/240 [00:01<00:00, 144.67it/s]\u001b[A\n",
      " 79%|███████████████████████████████▌        | 189/240 [00:01<00:00, 145.32it/s]\u001b[A\n",
      " 85%|██████████████████████████████████      | 204/240 [00:01<00:00, 143.51it/s]\u001b[A\n",
      " 91%|████████████████████████████████████▌   | 219/240 [00:01<00:00, 138.32it/s]\u001b[A\n",
      "100%|████████████████████████████████████████| 240/240 [00:01<00:00, 147.76it/s]\n",
      "04/28/2021 18:03:34 - INFO - utils_qa -   Saving predictions to ./outputs/train_dataset/exp1_test_10epochs/predictions.json.\n",
      "04/28/2021 18:03:34 - INFO - utils_qa -   Saving nbest_preds to ./outputs/train_dataset/exp1_test_10epochs/nbest_predictions.json.\n",
      "100%|███████████████████████████████████████████| 73/73 [00:10<00:00,  7.16it/s]\n"
     ]
    }
   ],
   "source": [
    "model_output_dir = increment_path(\"./models/train_dataset\", \"/exp\", exp_name)\n",
    "eval_output_dir = increment_path(\"./outputs/train_dataset\", \"/exp\", exp_name)\n",
    "!python train.py\\\n",
    "    --do_eval\\\n",
    "    --output_dir $eval_output_dir\\\n",
    "    --model_name_or_path /opt/ml/code/models/train_dataset/exp1_test_10epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model is from /opt/ml/code/models/train_dataset/exp1_test_10epochs\n",
      "data is from ../input/data/test_dataset\n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question'],\n",
      "        num_rows: 600\n",
      "    })\n",
      "})\n",
      "Lengths of unique contexts : 56737\n",
      "Embedding pickle load.\n",
      "[query exhaustive search] done in 6.295 s\n",
      "Sparse retrieval: 100%|████████████████████| 600/600 [00:00<00:00, 25907.02it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.83s/ba]\n",
      "init trainer...\n",
      "100%|█████████████████████████████████████████| 168/168 [00:17<00:00,  9.32it/s]04/28/2021 17:47:43 - INFO - utils_qa -   Post-processing 600 example predictions split into 1341 features.\n",
      "\n",
      "  0%|                                                   | 0/600 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                        | 14/600 [00:00<00:04, 139.06it/s]\u001b[A\n",
      "  5%|██▏                                      | 32/600 [00:00<00:03, 148.58it/s]\u001b[A\n",
      "  8%|███▍                                     | 51/600 [00:00<00:03, 158.71it/s]\u001b[A\n",
      " 12%|████▊                                    | 70/600 [00:00<00:03, 165.58it/s]\u001b[A\n",
      " 14%|█████▊                                   | 85/600 [00:00<00:03, 159.50it/s]\u001b[A\n",
      " 18%|███████                                 | 106/600 [00:00<00:02, 170.46it/s]\u001b[A\n",
      " 21%|████████▍                               | 126/600 [00:00<00:02, 177.51it/s]\u001b[A\n",
      " 24%|█████████▌                              | 143/600 [00:00<00:02, 171.60it/s]\u001b[A\n",
      " 27%|██████████▋                             | 160/600 [00:00<00:02, 164.83it/s]\u001b[A\n",
      " 30%|███████████▊                            | 177/600 [00:01<00:02, 163.04it/s]\u001b[A\n",
      " 33%|█████████████▏                          | 197/600 [00:01<00:02, 167.47it/s]\u001b[A\n",
      " 36%|██████████████▎                         | 214/600 [00:01<00:02, 154.90it/s]\u001b[A\n",
      " 39%|███████████████▌                        | 234/600 [00:01<00:02, 165.52it/s]\u001b[A\n",
      " 42%|████████████████▊                       | 253/600 [00:01<00:02, 171.80it/s]\u001b[A\n",
      " 45%|██████████████████                      | 271/600 [00:01<00:02, 153.76it/s]\u001b[A\n",
      " 48%|███████████████████▎                    | 290/600 [00:01<00:01, 160.35it/s]\u001b[A\n",
      " 52%|████████████████████▋                   | 311/600 [00:01<00:01, 170.67it/s]\u001b[A\n",
      " 55%|█████████████████████▉                  | 329/600 [00:01<00:01, 172.71it/s]\u001b[A\n",
      " 58%|███████████████████████▏                | 348/600 [00:02<00:01, 176.19it/s]\u001b[A\n",
      " 61%|████████████████████████▌               | 368/600 [00:02<00:01, 181.76it/s]\u001b[A\n",
      " 64%|█████████████████████████▊              | 387/600 [00:02<00:01, 181.75it/s]\u001b[A\n",
      " 68%|███████████████████████████             | 406/600 [00:02<00:01, 183.87it/s]\u001b[A\n",
      " 71%|████████████████████████████▎           | 425/600 [00:02<00:00, 175.87it/s]\u001b[A\n",
      " 74%|█████████████████████████████▌          | 443/600 [00:02<00:00, 159.55it/s]\u001b[A\n",
      " 77%|██████████████████████████████▊         | 463/600 [00:02<00:00, 169.65it/s]\u001b[A\n",
      " 80%|████████████████████████████████        | 481/600 [00:02<00:00, 153.87it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▏      | 498/600 [00:02<00:00, 156.19it/s]\u001b[A\n",
      " 86%|██████████████████████████████████▍     | 516/600 [00:03<00:00, 161.43it/s]\u001b[A\n",
      " 90%|███████████████████████████████████▊    | 537/600 [00:03<00:00, 170.12it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████   | 555/600 [00:03<00:00, 165.01it/s]\u001b[A\n",
      " 96%|██████████████████████████████████████▏ | 573/600 [00:03<00:00, 167.99it/s]\u001b[A\n",
      "100%|████████████████████████████████████████| 600/600 [00:03<00:00, 169.30it/s]\n",
      "04/28/2021 17:47:47 - INFO - utils_qa -   Saving predictions to ./outputs/test_dataset/exp1_test_10epochs/predictions.json.\n",
      "04/28/2021 17:47:47 - INFO - utils_qa -   Saving nbest_preds to ./outputs/test_dataset/exp1_test_10epochs/nbest_predictions.json.\n",
      "No metric can be presented because there is no correct answer given. Job done!\n",
      "100%|█████████████████████████████████████████| 168/168 [00:23<00:00,  7.22it/s]\n"
     ]
    }
   ],
   "source": [
    "test_output_dir = increment_path(\"./outputs/test_dataset\", \"/exp\", exp_name)\n",
    "!python inference.py\\\n",
    "    --do_predict\\\n",
    "    --output_dir $test_output_dir\\\n",
    "    --dataset_name ../input/data/test_dataset\\\n",
    "    --model_name_or_path /opt/ml/code/models/train_dataset/exp1_test_10epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Traceback (most recent call last):\n  File \"train.py\", line 348, in <module>\n    main()\n  File \"train.py\", line 34, in main\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/hf_argparser.py\", line 196, in parse_args_into_dataclasses\n    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\nValueError: Some specified arguments are not used by the HfArgumentParser: ['--', 'eval_accumulation_steps', '1', '--evaluation_strategy', 'epoch', '--logging_strategy', 'steps', '--logging_steps', '500', '--save_strategy', 'epoch', '--save_steps', '500', '--save_total_limit', '3', '--load_best_model_at_end', 'False', '--learning_rate', '5e-5', '--lr_scheduler_type', 'constant_with_warmup', '--warmup_ratio', '0.0', '--warmup_steps', '0', '--weight_decay', '0', '--adam_beta1', '0.9', '--adam_beta2', '0.999', '--adam_epsilon', '1e-8', '--adafactor', 'False', '--max_grad_norm', '1.0', '--group_by_length', 'False', '--label_smoothing_factor', '0.0', '--do_train']\n"
     ]
    }
   ],
   "source": [
    "!python train.py\\\n",
    "    --run_name None\\\n",
    "    --output_dir ./models/train_dataset --overwrite_output_dir False\\\n",
    "    --seed 42\\\n",
    "    --fp16 True --fp16_opt_level \"O1\"\\\n",
    "    --dataloader_num_workers 4\\\n",
    "    --dataloader_pin_memory True\\\n",
    "    --dataloader_drop_last False\\\n",
    "    \\\n",
    "    --num_train_epochs 10.0\\\n",
    "    --per_device_train_batch_size 16 --gradient_accumulation_steps 1\\\n",
    "    --per_device_eval_batch_size 16 -- eval_accumulation_steps 1\\\n",
    "    --evaluation_strategy \"epoch\"\\\n",
    "    --logging_strategy \"steps\" --logging_steps 500\\\n",
    "    --save_strategy \"epoch\"\\\n",
    "    --save_steps 500 --save_total_limit 3\\\n",
    "    --load_best_model_at_end False\\\n",
    "    \\\n",
    "    --learning_rate 5e-5 --lr_scheduler_type \"constant_with_warmup\"\\\n",
    "    --warmup_ratio 0.0 --warmup_steps 0\\\n",
    "    --weight_decay 0 --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-8\\\n",
    "    --adafactor False\\\n",
    "    --max_grad_norm 1.0\\\n",
    "    \\\n",
    "    --group_by_length False\\\n",
    "    --label_smoothing_factor 0.0\\\n",
    "    \\\n",
    "    --do_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EvaluationStrategy, IntervalStrategy(ExplicitEnum):\n",
    "    NO = \"no\"\n",
    "    STEPS = \"steps\"\n",
    "    EPOCH = \"epoch\"\n",
    "\n",
    "# class SchedulerType(ExplicitEnum):\n",
    "    LINEAR = \"linear\"\n",
    "    COSINE = \"cosine\"\n",
    "    COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n",
    "    POLYNOMIAL = \"polynomial\"\n",
    "    CONSTANT = \"constant\"\n",
    "    CONSTANT_WITH_WARMUP = \"constant_with_warmup\"\n",
    "\n",
    "# fp16_opt_level: https://nvidia.github.io/apex/amp.html"
   ]
  }
 ]
}