import faiss
from rank_bm25 import BM25Plus

from tqdm.auto import tqdm
import pandas as pd
import pickle
import json
import os
import numpy as np

from datasets import (
    Dataset,
    load_from_disk,
    concatenate_datasets,
)
from konlpy.tag import Mecab

import time
from contextlib import contextmanager


@contextmanager
def timer(name):
    t0 = time.time()
    yield
    print(f'[{name}] done in {time.time() - t0:.3f} s')


class Bm25SparseRetrieval:
    def __init__(self, tokenize_fn, data_path="../input/data/", context_path="wikipedia_documents.json"):
        self.data_path = data_path
        with open(os.path.join(data_path, context_path), "r") as f:
            wiki = json.load(f)

        # wikipedia_documentsëŠ” 'ì¸ë±ìŠ¤ ë²ˆí˜¸'ë¥¼ key ê°’ìœ¼ë¡œ ê°–ê³  dictë¥¼ valueë¡œ ê°–ëŠ” json íŒŒì¼ì´ë‹¤.(60613ê°œ)
        # ê° dictëŠ” text, corpus_source, url, domain, title, author, html, document_idë¥¼ key ê°’ìœ¼ë¡œ ê°–ëŠ”ë‹¤.
        # wikipedia_documentsì—ì„œ contextì— í•´ë‹¹í•˜ëŠ” text ë¶€ë¶„ì„ ë°›ì•„ì™€ì„œ ë¦¬ìŠ¤íŠ¸í™” í•œë‹¤.
        self.contexts = list(dict.fromkeys(
            set(v['text'] for v in wiki.values())))  # set ì€ ë§¤ë²ˆ ìˆœì„œê°€ ë°”ë€Œë¯€ë¡œ(list(dict)ëŠ” keyê°’ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“ ë‹¤.)
        print(f"Lengths of unique contexts : {len(self.contexts)}")
        self.ids = list(range(len(self.contexts)))

        # contextë¡œ í•™ìŠµëœ bm25 ê°ì²´ ìƒì„± ë° pickleë¡œ ì €ì¥
        self.bm25 = initialize_bm25_object()


        # should run get_sparse_embedding() or build_faiss() first.
        self.bm25v = None
        self.p_embedding = None
        self.indexer = None
        
    def initialize_bm25_object():
        # class initializerì˜ tokenize_fnì€ í˜•íƒœì†Œ ê¸°ë°˜ì¸ mecab.morphs(text)ì´ë‹¤.
        tokenized_contexts = [tokenize_fn(context) for context in self.contexts]
        bm25 = BM25Plus(tokenized_contexts)
        # pickleë¡œ bm25 ê°ì²´ ì €ì¥
        with open('bm25_object.pickle', 'wb') as f:
            pickle.dump(bm25, f)
        return bm25
    
    def get_sparse_embedding(self):
        # Pickle save.
        # /input/data/ì—ëŠ” dummy_dataset, test_dataset, train_dataset, wikipedia_document ì¡´ì¬
        emd_path = os.path.join(self.data_path, "sparse_embedding.bin")
        bm25v_path = os.path.join(self.data_path, "bm25v.bin")
        
        # ë§Œë“¤ì–´ë‘”ê²Œ ì¡´ì¬í•˜ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì•„ë‹ˆë©´ ìƒˆë¡œ ë§Œë“ ë‹¤.
        if os.path.isfile(emd_path) and os.path.isfile(tfidfv_path):
            with open(emd_path, "rb") as file:
                self.p_embedding = pickle.load(file)  # ê¸°ì¡´ì— ë§Œë“  sparse embedding ë²¡í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.
            with open(bm25v_path, "rb") as file:
                self.bm25v = pickle.load(file)
            print("Embedding pickle loadedğŸ‘")
        else:
            print("Building passage embedding...ğŸ¤”")
            self.p_embedding = self.tfidfv.fit_transform(
                self.contexts)  # ë¬¸ì„œ ì „ì²´ì— í•´ë‹¹í•˜ëŠ” termë“¤ì„ ì •ì˜í•˜ê³  idfë„ ê³„ì‚°
            print(f'P embedding shape: {self.p_embedding.shape}')
            
            with open(emd_path, "wb") as file:
                pickle.dump(self.p_embedding, file)
            with open(tfidfv_path, "wb") as file:
                pickle.dump(self.bm25v, file)
            print("Embedding pickle savedğŸ‘")

    def retrieve(self, dataset, topk=1):
        assert self.p_embedding is not None, "You must build faiss by self.get_sparse_embedding() before you run self.retrieve()."
        assert isinstance(dataset, Dataset), "dataset is not instance of DatasetğŸ‘¾"

        # make retrieved result as dataframe
        total = []
        for data in tqdm(dataset, desc='BM25 retrieval processingğŸ‘¾'):
            scores = self.bm25.get_scores(tokenize(data['question']))
            topk_indices = np.argsort(scores)[::-1][:topk]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•´ì„œ topkê°œì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ
            topk_retrieved = [self.contexts[i] for i in topk_indices]  # ì¶”ì¶œí•œ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” context ë½‘ì•„ì˜¤ê¸°

            tmp = {
                "question": data["question"],
                "id": data['id'],
                "context_id": topk_indices,  # retrieved id
                "context": topk_retrieved  # retrieved document
            }

            if 'context' in data.keys() and 'answers' in data.keys():
                tmp["original_context"] = data['context']  # original document
                tmp["answers"] = data['answers']  # original answer
            total.append(tmp)

        cqas = pd.DataFrame(total)
        return cqas

    def get_relevant_doc(self, query, k=1):
        """
        ì°¸ê³ : vocab ì— ì—†ëŠ” ì´ìƒí•œ ë‹¨ì–´ë¡œ query í•˜ëŠ” ê²½ìš° assertion ë°œìƒ (ì˜ˆ) ë™£ë™‡?
        """
        with timer("transform"):
            query_vec = self.tfidfv.transform([query])
        assert (
            np.sum(query_vec) != 0
        ), "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ ì˜¤ë¥˜ëŠ” ë³´í†µ queryì— vectorizerì˜ vocabì— ì—†ëŠ” ë‹¨ì–´ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤."

        with timer("query ex search"):
            result = query_vec * self.p_embedding.T
        if not isinstance(result, np.ndarray):
            result = result.toarray()
        sorted_result = np.argsort(result.squeeze())[::-1]
        return result.squeeze()[sorted_result].tolist()[:k], sorted_result.tolist()[:k]

    def get_relevant_doc_bulk(self, queries, k=1):
        query_vec = self.tfidfv.transform(queries)
        assert (
            np.sum(query_vec) != 0
        ), "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ ì˜¤ë¥˜ëŠ” ë³´í†µ queryì— vectorizerì˜ vocabì— ì—†ëŠ” ë‹¨ì–´ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤."

        result = query_vec * self.p_embedding.T
        if not isinstance(result, np.ndarray):
            result = result.toarray()
        doc_scores = []
        doc_indices = []
        for i in range(result.shape[0]):
            sorted_result = np.argsort(result[i, :])[::-1]
            doc_scores.append(result[i, :][sorted_result].tolist()[:k])
            doc_indices.append(sorted_result.tolist()[:k])
        return doc_scores, doc_indices

if __name__ == "__main__":
    # Test sparse
    org_dataset = load_from_disk("../input/data/data/train_dataset")
    full_ds = concatenate_datasets(
        [
            org_dataset["train"].flatten_indices(),
            org_dataset["validation"].flatten_indices(),
        ]
    )  # train dev ë¥¼ í•©ì¹œ 4192 ê°œ ì§ˆë¬¸ì— ëŒ€í•´ ëª¨ë‘ í…ŒìŠ¤íŠ¸
    print("*"*40, "query dataset", "*"*40)
    print(full_ds)

    ### Mecab ì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ê¸°ì— mecab ìœ¼ë¡œ ì„ íƒ í–ˆìŠµë‹ˆë‹¤ ###
    mecab = Mecab()

    def tokenize(text):
        return mecab.morphs(text)

    wiki_path = "wikipedia_documents.json"
    retriever = SparseRetrieval(
        tokenize_fn=tokenize,
        data_path="data",
        context_path=wiki_path)

    # test single query
    query = "ëŒ€í†µë ¹ì„ í¬í•¨í•œ ë¯¸êµ­ì˜ í–‰ì •ë¶€ ê²¬ì œê¶Œì„ ê°–ëŠ” êµ­ê°€ ê¸°ê´€ì€?"

    with timer("single query by exhaustive search"):
        scores, indices = retriever.retrieve(query)
    with timer("single query by faiss"):
        scores, indices = retriever.retrieve_faiss(query)

    # test bulk
    with timer("bulk query by exhaustive search"):
        df = retriever.retrieve(full_ds)
        df['correct'] = df['original_context'] == df['context']
        print("correct retrieval result by exhaustive search",
              df['correct'].sum() / len(df))
    with timer("bulk query by exhaustive search"):
        df = retriever.retrieve_faiss(full_ds)
        df['correct'] = df['original_context'] == df['context']
        print("correct retrieval result by faiss",
              df['correct'].sum() / len(df))
